{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T03:11:52.554217Z",
     "start_time": "2020-06-18T03:11:52.548391Z"
    }
   },
   "source": [
    "Compute the Annual Search Fraction similarity as described by Lee et al. (2015)\n",
    "\n",
    "    Lee, C.M.C., et al., Search-based peer firms: Aggregating investor perceptions through internet co-searches. Journal of Financial Economics (2015).\n",
    "    \n",
    "Because the calculation can be broken down to the daily level, for the purpose of memory efficiency, we will count unique combinations of `(ip, cik, next cik)` for each day, and then sum the daily counts to the annual level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T19:32:40.082287Z",
     "start_time": "2020-06-19T19:32:40.075297Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import os\n",
    "import dask\n",
    "import bs4\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO, BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T03:37:57.599396Z",
     "start_time": "2020-06-18T03:37:57.594932Z"
    }
   },
   "outputs": [],
   "source": [
    "# store large data files in separated dir\n",
    "from Utility.dir_LF import large_file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T19:27:36.853415Z",
     "start_time": "2020-06-19T19:27:36.850530Z"
    }
   },
   "outputs": [],
   "source": [
    "save_fp = large_file_dir  # set up file path\n",
    "#open(save_fp, 'wb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save log zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get url list of one year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T19:20:47.102902Z",
     "start_time": "2020-06-19T19:20:47.097998Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    hack  = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    r = requests.get(url, timeout = 30, headers = hack)\n",
    "    r.encoding = 'utf-8'      \n",
    "    return bs4.BeautifulSoup(r.text)\n",
    "\n",
    "def get_year_log_url(year):\n",
    "    url_temp =  'https://www.sec.gov/files/edgar' + str(year) +'.html'\n",
    "    soup = get_soup(url_temp)\n",
    "    urls = []\n",
    "    links = soup.findAll(\"a\")\n",
    "    for link in links:\n",
    "        urls.append(link[\"href\"])    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download each day (with dask decorator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T00:11:02.355668Z",
     "start_time": "2020-06-20T00:11:02.351074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download all log zip files\n",
    "@dask.delayed # Why delay here? ----Break up computations into many pieces\n",
    "def get_day_log(log_url, save_fp):\n",
    "    \n",
    "    sleep_time = 1 # sleep\n",
    "    \n",
    "    hack  = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    r = requests.get(log_url, headers = hack)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        with open(save_fp, 'wb') as f:  \n",
    "            f.write(r.content)\n",
    "            \n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "    return log_url, r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T01:17:10.317362Z",
     "start_time": "2020-06-20T01:17:10.304193Z"
    }
   },
   "outputs": [],
   "source": [
    "# based on get_day_log\n",
    "\n",
    "def get_year_logs(year):  # only 2003~2017 \n",
    "    \n",
    "    #https://www.sec.gov/dera/data/edgar-log-file-data-set.html\n",
    "    '''\n",
    "    download all daily log files for one year from\n",
    "    https://www.sec.gov/dera/data/edgar-log-file-data-set.html\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # save path\n",
    "    save_dir = f\"{large_file_dir}/log/{year}\" # f\"...\", {vars}\n",
    "\n",
    "    if not os.path.isdir(save_dir): # check if save_dir exists\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    # get log csv\n",
    "    url_lst = get_year_log_url(year)\n",
    "    \n",
    "    task_done = []\n",
    "    task_error = []\n",
    "    task_lst = [] # an iterable storing delayed objects (storing many callings, then parallelly run them together)\n",
    "    \n",
    "    for url in url_lst:\n",
    "        save_f = save_dir +'/'+ url[-15:-4] + '.zip'\n",
    "                \n",
    "        ###### key feature of multitasking\n",
    "            # skip if already exists\n",
    "        if os.path.isfile(save_f):\n",
    "            continue # force to next iteration \n",
    "        \n",
    "        task_lst.append(get_day_log(url, save_f))     # calling func does not actually run it\n",
    "                                                    # Why? ---Compute on lots of computation at once(avoid         \n",
    "                                                        # calling compute() repeatly)\n",
    "    \n",
    "    with ProgressBar():    # actually run it\n",
    "        task_lst = dask.compute(task_lst)\n",
    "    \n",
    "    try: \n",
    "        task_done = [(url, code) for url, code in task_lst if code==200]\n",
    "        task_error = [(url, code) for url, code in task_lst if code!=200]\n",
    "        print('failed urls', task_error, sep='\\n')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \n",
    "\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T01:17:33.368598Z",
     "start_time": "2020-06-20T01:17:10.319739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 22.9s\n"
     ]
    }
   ],
   "source": [
    "get_year_logs(2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T19:35:54.861587Z",
     "start_time": "2020-06-19T19:35:54.859124Z"
    }
   },
   "source": [
    "# get indeces before a certain year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download a qtr (with dask decorator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T20:49:05.659241Z",
     "start_time": "2020-06-19T20:49:05.645654Z"
    }
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def download_masterindex(year, qtr):   # Date should never forward the latest log accord\n",
    "    from urllib.request import urlopen\n",
    "    # Download Master.idx from EDGAR\n",
    "    # Loop accounts for temporary server/ISP issues\n",
    "    # ND-SRAF / McDonald : 201606\n",
    "\n",
    "    number_of_tries = 10\n",
    "    sleep_time = 10  # Note sleep time accumulates according to err\n",
    "\n",
    "    PARM_ROOT_PATH = 'https://www.sec.gov/Archives/edgar/full-index/'\n",
    "\n",
    "    masterindex = []\n",
    "    #  using the zip file is a little more complicated but orders of magnitude faster\n",
    "    append_path = str(year) + '/QTR' + str(qtr) + '/master.zip'  # /master.idx => nonzip version\n",
    "    sec_url = PARM_ROOT_PATH + append_path\n",
    "\n",
    "    for i in range(1, number_of_tries + 1):\n",
    "        try:\n",
    "            zipfile = ZipFile(BytesIO(urlopen(sec_url).read()))\n",
    "            records = zipfile.open('master.idx').read().decode('utf-8', 'ignore').splitlines()[9:]\n",
    "            break\n",
    "        except Exception as exc:\n",
    "            if i == 1:\n",
    "                print('\\nError in download_masterindex')\n",
    "            print('  {0}. _url:  {1}'.format(i, sec_url))\n",
    "\n",
    "            print('  Warning: {0}  [{1}]'.format(str(exc), time.strfime('%c')))\n",
    "            if '404' in str(exc):\n",
    "                break\n",
    "            if i == number_of_tries:\n",
    "                return False\n",
    "            print('     Retry in {0} seconds'.format(sleep_time))\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "    index_str = '\\n'.join([record for i, record in enumerate(records) if i!=1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # save\n",
    "    save_dir = f'{large_file_dir}/masterindex/'\n",
    "    if not os.path.isdir(save_dir): # check if save_dir exists\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    \n",
    "    \n",
    "    save_f = f\"{large_file_dir}/masterindex/mi_{year}_{qtr}.csv\"\n",
    "    if os.path.isfile(save_f):\n",
    "          pass\n",
    "    elif index_str == False:\n",
    "        print('error occurs on',year,qtr)\n",
    "    else:\n",
    "        index_pd = pd.read_csv(StringIO(index_str), sep='|')\n",
    "        index_pd.to_csv(save_f, index=False)\n",
    "           \n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download all indeces before a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T00:08:53.989206Z",
     "start_time": "2020-06-20T00:08:53.984581Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def download_all_index(year): # return a df contains all index before year\n",
    "\n",
    "    dask_lst = []  # each year, each element\n",
    "    \n",
    "    for year in range(1995, year+1):  # +1\n",
    "        for qtr in range(1, 5):\n",
    "            #print(\"processing\", year, 'qtr',qtr)\n",
    "            dask_lst.append(download_masterindex(year, qtr))\n",
    "    \n",
    "    dask_lst = dask.compute(dask_lst)\n",
    "    \n",
    "    with ProgressBar():    # actually run it\n",
    "        dask_lst = dask.compute(dask_lst)\n",
    "    \n",
    "    return\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T00:14:16.540833Z",
     "start_time": "2020-06-20T00:14:13.845073Z"
    }
   },
   "outputs": [],
   "source": [
    "download_all_index(2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
