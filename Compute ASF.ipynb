{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:03:57.199704Z",
     "start_time": "2020-06-19T21:03:57.189632Z"
    }
   },
   "source": [
    "Compute the Annual Search Fraction similarity as described by Lee et al. (2015)\n",
    "\n",
    "    Lee, C.M.C., et al., Search-based peer firms: Aggregating investor perceptions through internet co-searches. Journal of Financial Economics (2015).\n",
    "    \n",
    "Because the calculation can be broken down to the daily level, for the purpose of memory efficiency, we will count unique combinations of `(ip, cik, next cik)` for each day, and then sum the daily counts to the annual level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T03:33:19.449067Z",
     "start_time": "2020-06-20T03:33:19.426829Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import dask\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "from calendar import monthrange\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:08:02.025062Z",
     "start_time": "2020-06-19T21:08:02.021434Z"
    }
   },
   "outputs": [],
   "source": [
    "# store large data files in separated dir\n",
    "from Utility.dir_LF import large_file_dir\n",
    "\n",
    "# keep form \n",
    "from Utility.Form_keep import keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masterindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T02:13:24.146956Z",
     "start_time": "2020-06-20T02:13:24.140450Z"
    }
   },
   "outputs": [],
   "source": [
    "def append_mi_pd(year):\n",
    "    pd_lst = []\n",
    "    for y in range(1995,year): \n",
    "        print('appending',y)\n",
    "        for quarter in range(1,5):\n",
    "            \n",
    "            mi_fp = f\"{large_file_dir}/masterindex/mi_{y}_{quarter}.csv\"\n",
    "            mi_pd = pd.read_csv(mi_fp)\n",
    "            mi_pd['accession']= mi_pd.apply(lambda row: row['Filename'].split('.txt')[0].split('/')[3], axis = 1) # create new column with accession\n",
    "            pd_lst.append(mi_pd)\n",
    "            \n",
    "    mi_pd = pd.concat(pd_lst, axis=0, ignore_index=True)\n",
    "    mi_pd = mi_pd[['accession', 'Form Type']].drop_duplicates()\n",
    "    mi_pd.to_csv(f\"data/mis{year}.csv\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T02:14:38.277453Z",
     "start_time": "2020-06-20T02:13:24.902571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending 1995\n",
      "appending 1996\n",
      "appending 1997\n",
      "appending 1998\n",
      "appending 1999\n",
      "appending 2000\n",
      "appending 2001\n",
      "appending 2002\n",
      "appending 2003\n"
     ]
    }
   ],
   "source": [
    "append_mi_pd(2004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP1500 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T00:24:30.477031Z",
     "start_time": "2020-06-20T00:24:30.468278Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSPindex(day):\n",
    "    '''\n",
    "    get the S&P index as of `day`\n",
    "    \n",
    "    day: a python datetime instance\n",
    "    \n",
    "    return two dictionaries\n",
    "     sp_d = {\n",
    "       500: set(int,...), # set of cik firms that are in the S&P 500 universe\n",
    "       1500: set(int,...) # set of cik firms that are in the S&P 1500 universe\n",
    "     }\n",
    "     cik2gvkey_d = {\n",
    "       500: a pandas series with cik as index and gvkey as values, \n",
    "            the map from cik to gvkey in S&P500\n",
    "       1500: a pandas series with cik as index and gvkey as values, \n",
    "            the map from cik to gvkey in S&P1500\n",
    "     }\n",
    "    '''\n",
    "    # see `./data/index_constituents-compustat_north_america.pdf` for how\n",
    "    # 'SPIndex.csv' is generated and should be used\n",
    "    sp_index_pd = pd.read_csv('./data/SPIndex.csv', parse_dates=['from', 'thru'])\n",
    "    sp_500_pd = sp_index_pd.loc[sp_index_pd.tic=='I0003',:]\n",
    "    sp_1500_pd = sp_index_pd.loc[sp_index_pd.tic=='I0020',:]\n",
    "    sp_d = {}\n",
    "    cik2gvkey_d = {}\n",
    "    for universe_size, tmp in zip([500, 1500], [sp_500_pd, sp_1500_pd]):\n",
    "        tmp = tmp[(tmp['from'] <= day) & ((tmp.thru >= day)|tmp.thru.isna())]\n",
    "        # S&P 1500 at a given timepoint might not consist of exactly 1500 unique cik firms\n",
    "        # but should roughly be 1500, the same for S&P 500\n",
    "        # assert tmp.co_cik.nunique() == universe_size\n",
    "        tmp2 = tmp.co_cik.unique().tolist() # only get cik\n",
    "        sp_co_cik_l = [int(x) for x in tmp2 if str(x) != 'nan']\n",
    "        sp_d[universe_size] = sp_co_cik_l\n",
    "        tmp3 = tmp[['gvkey', 'co_cik']].drop_duplicates()\n",
    "        tmp3 = tmp3.set_index('co_cik')\n",
    "        cik2gvkey_d[universe_size] = tmp3\n",
    "    return sp_d, cik2gvkey_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T00:24:59.548684Z",
     "start_time": "2020-06-20T00:24:59.488363Z"
    }
   },
   "outputs": [],
   "source": [
    "sp_d, cik2gvkey_d = getSPindex(datetime(2008,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T01:20:15.883851Z",
     "start_time": "2020-06-20T01:20:15.877944Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_daily_log_pd(day):\n",
    "    '''\n",
    "    day: python datetime instance\n",
    "    '''\n",
    "    save_fp = f\"{large_file_dir}/log/{day.year}/log{day.strftime('%Y%m%d')}.zip\"\n",
    "    with ZipFile(save_fp) as f:\n",
    "        which = f.namelist()[0]\n",
    "        df = pd.read_csv(BytesIO(f.read(which)), encoding='utf-8')\n",
    "        \n",
    "    df['date'] = df['date'].astype('datetime64[ns]')\n",
    "    df['cik'] = df['cik'].astype(int)\n",
    "    df.drop(['zone','extention','code','size','idx','norefer','noagent','find','browser'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T02:15:30.240558Z",
     "start_time": "2020-06-20T02:15:27.999124Z"
    }
   },
   "outputs": [],
   "source": [
    "mis = pd.read_csv('data/mis2004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T03:03:04.425Z"
    }
   },
   "outputs": [],
   "source": [
    "# have to prepare global vars: sp_d, keep\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def get_pairs_day(day):\n",
    "    \n",
    "    '''\n",
    "    day: a datetime instance\n",
    "    e.g. : datetime(2008, 1, 1)\n",
    "    '''\n",
    "    \n",
    "    df = get_daily_log_pd(day)\n",
    "    if df.shape[0] != 0:\n",
    "    #print('processing'+str())\n",
    "        \n",
    "    \n",
    "        df = df.merge(mis, on='accession')  # get Form Type from master index(1995-2011)\n",
    "# Step2 : SP1500\n",
    "        df = df[df.cik.isin(sp_d[1500])]\n",
    "\n",
    "# Step3&4 : unique cik >1, cik<50\n",
    "        df_freq = df.groupby(by='ip', as_index=False).agg({'cik': pd.Series.nunique})\n",
    "        df_freq.columns = ['ip','uniq_search']  # unique cik search for each ip\n",
    "        df = df.merge(df_freq, on='ip')\n",
    "        del df_freq\n",
    "        df = df.drop(df[(df['uniq_search'] > 50) | (df['uniq_search'] <= 1)].index)\n",
    "\n",
    "# Step5: keep certain forms searches\n",
    "    \n",
    "        df = df[df['Form Type'].isin(keep)]\n",
    "\n",
    "# Step5a: filter again those only 1 obs \n",
    "        #* this was not mentioned in Table 1, but illustrated in Table A1 (reason for dropping observation 3)\n",
    "\n",
    "        df_freq2 = df.groupby(by='ip', as_index=False).agg({'cik': pd.Series.nunique})\n",
    "        df_freq2.columns = ['ip','uniq_search2']  # unique cik search for each ip\n",
    "        df = df.merge(df_freq2, on='ip')\n",
    "        del df_freq2\n",
    "        df = df.drop(df[(df['uniq_search2'] <= 1)].index)\n",
    "        df.drop(['uniq_search','crawler'],axis= 1, inplace = True)  # drop some unused vars\n",
    "\n",
    "# Step 6a, Removing consecutive searches\n",
    "        df = df[['ip','date','time','cik']]  # naturally ordered by ip/date/time\n",
    "        df = df.groupby('ip').apply(\n",
    "            lambda x: x.loc[\n",
    "                x.cik.shift(-1) != x.cik\n",
    "                ]).reset_index(drop = True)\n",
    "\n",
    "#  step 6b, removing double-counted\n",
    "\n",
    "        ip_list = df['ip'].unique().tolist()\n",
    "\n",
    "        a_day_cik_link = []\n",
    "        for ip in ip_list:\n",
    "\n",
    "            df_temp = df[df['ip'] == ip]\n",
    "\n",
    "            temp_link = [] # a _user_cik_link\n",
    "            for i in range(len(df_temp['cik'].tolist())-1):\n",
    "                a_link = (df_temp['cik'].tolist()[i],df_temp['cik'].tolist()[i+1]) # tuple ,hashable\n",
    "                temp_link.append(a_link)\n",
    "            temp_link = list(set(temp_link)) # removing double-counted\n",
    "\n",
    "            a_day_cik_link = a_day_cik_link + temp_link\n",
    "        return a_day_cik_link\n",
    "    #cik_list_day = df['cik'].unique().tolist()\n",
    "    #print('sample pairs', a_day_cik_link[0:10])\n",
    "\n",
    "    # output is a day cik link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T03:03:05.528Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pairs_year(year): # Always Using SP1500 index of 2008 Jan 1\n",
    "\n",
    "    ayear_pairs = []\n",
    "\n",
    "    for month in range(1,13):\n",
    "        last_day = monthrange(year, month)[1]\n",
    "        for day in range(last_day):\n",
    "            try:\n",
    "                aday= get_pairs_day(datetime(year, month, day+1))\n",
    "                ayear_pairs = ayear_pairs + aday\n",
    "            except:\n",
    "                continue\n",
    "    with ProgressBar():   \n",
    "        ayear_pairs = dask.compute(ayear_pairs)[0]\n",
    "    \n",
    "    return ayear_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T02:56:20.719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df is empty in 20030101\n",
      "df is empty in 20030104\n",
      "df is empty in 20030105\n",
      "df is empty in 20030111\n",
      "df is empty in 20030112\n",
      "df is empty in 20030117\n",
      "df is empty in 20030118\n",
      "df is empty in 20030119\n",
      "df is empty in 20030120\n",
      "df is empty in 20030121\n",
      "df is empty in 20030122\n",
      "df is empty in 20030131\n",
      "df is empty in 20030201\n",
      "df is empty in 20030202\n",
      "df is empty in 20030203\n",
      "df is empty in 20030204\n",
      "df is empty in 20030205\n",
      "df is empty in 20030206\n",
      "df is empty in 20030207\n",
      "df is empty in 20030208\n",
      "df is empty in 20030209\n",
      "df is empty in 20030210\n",
      "df is empty in 20030211\n",
      "df is empty in 20030218\n"
     ]
    }
   ],
   "source": [
    "pairs2003 = get_pairs_year(2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asf(pairs, year):\n",
    "    df = pd.DataFrame(pairs, columns=['a', 'b'])\n",
    "    #df2008.info()\n",
    "    #  freq_a\n",
    "    df2 = df.groupby('a').size().reset_index(name=\"freq_a\")\n",
    "    #df2.info()\n",
    "# add freq_a\n",
    "    df3 = df2008.merge(df2, on=\"a\")\n",
    "    #df3.info()\n",
    "    # freq_ab\n",
    "    df4 = df3.groupby([\"a\", \"b\"]).size().reset_index(name=\"freq_ab\")\n",
    "    #df4.info()\n",
    "\n",
    "# add freq_ab\n",
    "    df5 = df3.merge(df4, on =['a','b'])\n",
    "    df5['asf']=df5['freq_ab']/df5['freq_a']\n",
    "    df5.drop(['freq_ab','freq_a'], axis= 1, inplace = True) \n",
    "    \n",
    "    df5.to_csv(f'asf{year}.csv',, sep='\\t', encoding='utf-8')\n",
    "    return df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_asf(pairs2003, 2003)\n",
    "df[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
